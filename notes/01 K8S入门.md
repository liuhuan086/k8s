# 1.3 K8S简介

## 1.3.1 Service与标签、标签选择器

在K8S中，Service是分布式集群架构的核心，一个Service对象拥有如下关键特征：

* 拥有一个唯一指定的名字
* 拥有一个虚拟IP（Cluster IP，Service IP或者VIP）和端口号。
* 能够提供某种远程服务的能力。
* 被映射到了提供这种服务能力的一组容器应用上。

Service的服务进程目前都基于socket通信方式对外提供服务，一个Service可能有多个相关服务进程，每个服务进程都有一个独立的Endpoint（IP+Port）访问点，K8S能够通过Service（虚拟Cluster IP+Service Port）连接到指定Service上。这样，即使服务进程的IP改变，通过Service一样能够找到对应服务进程。

容器提供了强大的隔离功能，因此有必要将为Service提供服务的这组进程放入容器中进行隔离。为此，K8S设计了Pod对象，将每个服务进程“包装”到相应的Pod中，使其成为Pod中运行的一个容器（Container）。为了建立Service和Pod之间的关联关系，K8S首先给每个Pod贴上一个标签（Label），然后给相应的Service定义标签选择器（Label Selector），比如MySQL Service的标签选择器的选择条件为`name=MySQL`，意味着该Service要作用于所有包含`name=MySQL`Label的Pod上。这样，就巧妙的解决了Service与Pod的关联问题。

## 1.3.2 集群节点

在集群管理方面，K8S将集群中的机器划分为一个Master节点和一群工作节点（Node）。其中，在master节点上运行着集群管理相关的一组进程：kube-apiserver、kube- controller- manager和kube-scheduleer，这些进程实现了自动对这个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错管理功能。

Node作为集群中的工作节点，运行真正的应用程序，Node上K8S管理的最小运行单元是Pod。Node上运行着K8S的kubelet、kube-proxy服务进程，这些服务进程负责Pod的创建、启动、监控、重启、销毁以及实现软件模式的负载均衡器。

![](https://borinboy.oss-cn-shanghai.aliyuncs.com/huan/k8s%E6%9E%B6%E6%9E%84.png)

**服务扩容与升级**

在K8S中，只需要为需要扩容的Service关联的Pod创建一个Replication Controller（RC）即可解决扩容及Service升级等问题。

一个RC文件中包含三个关键信息：

* 目标Pod的定义
* 目标Pod需要运行的副本数量（Eplicas）
* 要监控的目标Pod的标签（Label）

在创建好RC后，K8S会通过RC中定义的Lable筛选出对应的Pod实例并实时监控器状态和数量，如果实例数量少于定义的副本数量（Replicas），则会根据RC中定义的Pod模板来创建一个新的Pod，然后将此Pod调度到合适的Node上启动运行，知道Pod实例数量达到预定目标。这个过程完全是自动化的，无需人工干预。

**为什么选择K8S**

在K8S的架构方案中，底层网络的细节完全被屏蔽，甚至无需修改基于服务的Cluster IP，就能将系统从物理机运行环境无缝迁移到公有云中。且K8S系统架构具备了超强的动态扩容能力。

# 1.4 K8S基本概念和术语

在K8S中，Node、Pod、Replication Controller、Service等概念都可以看作一种资源对象，通过K8S提供的kubectl工具或者API调用进行操作，并保存在etcd中。

## 1.4.1 Master

K8S里的Master指的是集群控制节点，每个K8S集群里需要有一个Master节点来负责整个集群的管理和控制。

Master节点上运行着以下关键进程：

* API Server：提供了HTTP Rest接口的关键服务进程，是K8S里所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。
* Kubernetes Controller Manager：K8S里所有资源对象的自动化控制中心。
* K8S Scheduler：负责（Pod）资源调度的进程。

> 另外还需要etcd服务进程，因为K8S里的所有资源对象数据全都保存在etcd中的。

## 1.4.2 Node（节点）

Node节点是K8S集群中的工作负载节点（真正干活的），每个Node都会被Master分配一些工作负载（Docker容器）。在多个Node节点的环境中，当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上去。

每个Node节点运行着以下关键进程：

* kubelet：负责Pod对应的容器的创建、启动和停止等任务，同时与Master节点密切协作，相互通信，实现集群管理的基本功能。
* kube-proxy：实现K8S Service的通信与负载均衡机制的重要组件。
* Docker Engine：Docker引擎，负责本节点的容器创建和管理工作。

Node信息如下：

* Node地址：主机的IP地址、或者Node ID
* Node运行状态：包括Pending、Running、Terminated三种状态。
* Node Condition：描述Running状态Node的运行条件，目前只有一种条件——Ready。Ready表示Node处于健康状态，可以接收从Master发来的创建Pod的指令。
* Node系统容量：描述Node可用的系统资源，包括CPU、内存数量、最大可调度Pod数量等。
* 其他：Node其他信息，包括实例的内核版本号、K8S版本号、Docker版本号、操作系统名称等。

可以通过命令查看node详细信息：

```
kubectl describe node <node_name>
```

```
[root@hdss7-21 ~]# kubectl get node
NAME                STATUS   ROLES         AGE   VERSION
hdss7-21.host.com   Ready    master,node   14d   v1.17.2
hdss7-22.host.com   Ready    master,node   14d   v1.17.2

[root@hdss7-21 ~]# kubectl describe node hdss7-21.host.com
Name:               hdss7-21.host.com
Roles:              master,node
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=hdss7-21.host.com
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
                    node-role.kubernetes.io/node=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
..............................
此处省略
..............................
[root@hdss7-21 ~]#
```

### 1. Node管理

Node通常是物理机、虚拟机或云主机提供的资源，并不是由K8S创建的。使用K8S创建一个Node，只是表示K8S在系统内部创建了一个Node对象，创建后会对其进行一系列健康检查，如果检查不通过，则该Node将会在集群中被标记为不可用。

### 2. Node Controller

Node Controller是K8S Master中的一个组件，用于管理Node对象。功能：

* 集群范围内的Node信息同步
* 单个Node的生命周期管理

Node信息同步可以通过kube-controller-manager的启动参数`--node-sync-period`设置同步的时间周期。

### 3. Node的自注册

Kubelet的`--register-node`默认为true，kubelet会向apiserver注册自己。这也是K8S推荐的Node管理方式。Kubelet进行自注册的启动参数如下：

* --apiservers=：apiserver的地址
* --kubeconfig=：登录apiserver所需凭据/证书的目录
* -- cloud_provider=：云服务商地址，用于获取自身的metadata

### 4. 手动管理Node

K8S也可以收工创建和修改Node对象，需要将`--register-node`设为false。

## 1.4.3 Pod

Pod是K8S最基本的操作单元，包括一个或多个紧密相关的容器，一个Pod中的多个容器应用通常是紧耦合的，是在Node上被创建、启动或者销毁。

使用Pod而不直接使用Docker，很重要的一个原因是Docker容器之间的通信受到Docker网络机制的限制。

一个Pod中的应用容器共享同一组资源，如下：

* PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。
* 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。
* IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。
* UTS命名空间：Pod中的多个容器共享同一个主机名。
* Volumes（共享存储卷）：Pod中的各个容器可以访问在Pod级别定义的Volumes。

K8S为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。K8S要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信。

### Pause容器

每个Pod都有一个特殊的被称为“根容器”的Pause容器，这个Pause容器与业务无关且不易死亡，以它的状态代表整个容器组的状态，就解决了如何判断这一组容器的状态是否正常这个问题。

Pod容器中多个业务容器共享Pause容器的IP，共享Pause容器挂载的Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好的解决了它们之间共享文件的问题。

### 1. 对Pod的定义

对Pod的定义通过YAML或Json格式的配置文件来完成。Pod的生命周期是通过Replication Controller来管理的。Pod生命周期过程包括：

* Pending：Pod定义正确，提交到Master，但其所包含的容器镜像还未完全创建。通常Master对Pod进行调度需要一些时间，之后Node对镜像进行下载也需要一些时间。
* Running：Pod已被分配到某个Node上，且包含的所有容器镜像都已经创建完成，并成功运行起来。
* Successed：Pod中所有同期都成功结束，并且不会被重启，这是Pod的一种最终状态。
* Failed：Pod中所有容器都结束了，但至少一个容器是以失败状态结束的，这也是Pod的一种最终状态。

## 1.4.4 Label(标签)

Label是K8S系统的一个核心概念。Label以key/value键值对的形式附加到各种对象上。如Pod、Service、RC、Node等。Label定义了这些对象的可识别属性，用来对它们进行管理和选择。Label可以在创建对象时附加到对象上，也可以在对象创建后通过API进行管理。

在为对象定义好Label后，其他对象就可以使用Label Selector（选择器）来定义起作用的对象了。

Label Selector的定义由多个逗号分隔的条件组成。

```
"labels" : {
	"key1": "value1",
	"key2": "value2"
}
```

当前有两种Label Selector：基于等式的（Equality-based）和基于集合的（Set-based），在使用时可以将多个Label进行组合来选择。

一些常用Label示例：

* keys: ["release", "environment", "tier", "partiton", "track"]
* values: ["stable", "canary", "dev", "qa", "pro", "frontend", "backend", "middleware", "customerA", "customerB", "daily", "weekly"]

Label Selector在K8S中的重要使用场景：

* Replication Controller通过Label Selector来选择要管理的Pod。
* kube-proxy进程通过Service的Label Selector来选择对应的Pod，自动建立起每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。
* 通过对某些Node定义特定的Label，并且在Pod定义文件中使用NodeSelector这种标签调度策略，kube-scheduler进程可以实现Pod的”定向调度“特性。

看一下redis-slave RC的yaml配置文件，在RC的定义中，Pod部分的template.metadata.labels定义了Pod的Label，即name=redis-slave。然后在spec.selector中指定name=redis-slave，表示将对所有包含该Label的Pod进行管理。

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  replicas: 2
  selector:
    name: redis-slave
  template:
    metadata:
      labels:
        name: redis-slave
    spec:
      containers:
      - name: slave
        image: kubeguide/guestbook-redis-slave
        ports:
        - containerPort: 6379
        env:
        - name: GET_HOSTS_FROM
          value: env
```

同样，在Service的定义中，也通过定义spec.selector为name=redis-slave来选择将哪些具有该Label的Pod加入其Load Balance的后端列表中去。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    name: redis-slave
spec:
  selector:
    name: redis-slave
  ports:
  - port: 6379
```

使用Label可以给对象创建多组标签，Service、RC等组件则通过Label Selector来选择对象范围，Label和Label Selector共同构成了K8S系统中最核心的应用模型，使得被管理对象能够被精细地分组管理，同时实现了整个集群的高可用性。

## 1.4.5 Replication Controller（RC）

**在K8S较早的版本中**，Replication Controller是K8S系统中的核心概念，用于定义Pod副本的数量，保证集群中运行着用户期望的副本数量。在Master中，RC进程通过RC的定义来完成Pod的创建、监控、启停等操作。

对RC的定义使用YAML或JSON格式的配置文件来完成。在K8S中，可以使用`kubectl scale`命令来一键完成。

注意：删除RC并不会影响通过该RC已创建好的Pod。为了删除所有Pod，可以设置replicas值为0，然后更新该RC。也可以用stop和delete命令来一次性删除RC和RC控制的全部Pod。

当应用升级时，通常会通过Build一个新的Docker镜像，并用新的Docker镜像版本来替代旧的版本以实现升级。就可以通过停掉一个旧的Pod，启动一个新的Pod的方式实现平滑升级。

**在目前的新版本中**，Replica Set与Deployment这两个资源对象逐步替换了之前RC的作用。

## 1.4.6 Deployment

是K8S v1.2版本中引入的新概念，引入的目的是为了更好的解决Pod的编排问题，是通过Replica Set来实现的。使用场景如下：

* 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。
* 检查Deployment的状态来查看部署动作是否完成（Pod数量是否达到预期）。
* 更新Deployment以创建新的Pod（比如镜像升级）。
* 如果当前Deployment存在问题，则可以会滚到上一个Deployment版本。
* 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再回复Deployment，进行新的发布。
* 扩展Deployment以应对高负载。
* 清理不再需要的旧版本ReplicasSets。

## 1.4.8 StatefulSet

在K8S系统中，Pod的管理对象RC、Deployment、DaemonSet和Job都是面向无状态服务。但现实中很多服务都是有状态的，特别是一些复杂的中间件集群，例如MySQL集群、MongoDB集群等，这些集群有一些共同点：

* 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以互相发现并通信。
* 集群的规模比较固定，且不能随意变动。
* 集群里的每个节点都是有状态的，通常会持久化数据到硬盘中。
* 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损。

因此，如果我们使用RC/Deployment控制副本数的方式来实现上述有状态的集群，则第一点是无法满足的，因为Pod的ID是随机生成的。另外，为了能够在其他节点上恢复某个失败的节点，这种集群中的Pod需要挂载到某种共享存储。为了解决这个问题，K8S从v1.4版本引入了PetSet这个新的资源对象，并在v1.5版本中更名为StatefulSet。StatefulSet从本质上来说，可以看作是RC/Deployment的变种，它具有以下特性：

* StatefulSet里的每个Pod都有稳定的、唯一的网络标识，可以用来发现集群中的其他成员。假设StatefulSet中的名字是Kafka，那么第一个Pod叫Kafka-0，第二个叫Kafka-1，并以此类推。
* StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个的Pod已经是运行且就绪状态。
* StatefulSet里的Pod采用稳定的持久化存储，通过PV/PVC来实现，删除Pod时默认不会删除StatefulSet相关的存储卷。

StatefulSet除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet的定义中要声明它是属于哪个Headless Service。Headless Service与普通Service的关键区别在于，它没有Cluster IP，如果解析Headless Service的DNS域名，则返回的是该Service对应的全部Pod的EndPoint列表。StatefulSet在Headless Service的i初上，又为StatefulSet控制的每个Pod实例创建了一个DNS域名。

## 1.4.9 Service

### 1. Service的定义

Service也是K8S中最核心的资源对象之一，**K8S里的每个Service其实就是我们经常提起的微服务架构中的一个“微服务”**。每个Service都分配了一个全局唯一的虚拟IP地址，一旦创建就不会改变，这个虚拟IP被称为Cluster IP，这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就成为了最基础的TCP网络通信问题，即使Pod的IP发生改变，也不会产生任何影响。

每个Pod都有一个唯一的IP，且每个Pod都提供了一个独立的Endpoint（Pod IP + ContainerPort）以供客户端访问，多个Pod副本组成的集群，客户端需要如何访问它们呢？一般的做法是部署一个负载均衡器（kube-proxy），为这组Pod开启一个对外的服务端口，如8000端口，并且将这些Pod的Endpoint列表加入8000端口转发列表中，客户端就可以通过负载均衡器的对外IP地址+服务端口来访问服务，被转发到哪个Pod上，则由负载均衡算法来决定。

#### Endpoint对象

Endpoint对象主要由Pod和IP地址和容器需要监听的端口号组成，使用命令可以查看：

```
[root@hdss7-21 ~]# kubectl get endpoints
NAME           ENDPOINTS                                   AGE
frontend       172.7.21.8:80,172.7.22.7:80,172.7.22.8:80   5h23m
redis-master   172.7.21.6:6379,172.7.22.5:6379             5h40m
redis-slave    172.7.21.7:6379,172.7.22.6:6379             5h40m
```

一个Service可以看作一组提供相同服务的Pod的对外访问接口。Service作用于哪些Pod是通过Label Selector来定义的。

可以继续查看redis-slave的yaml文件，K8S将会创建一个名为“redis-slave”的服务，并在6479端口上监听。spec.selector的定义表示该Service将包含所有具有“name=redis-slave” Label的Pod。

### 2. IP相关

Pod的IP地址是Docker Daemon根据docker0网桥的IP地址段进行分配的，但Service的Cluster IP地址是K8S系统中的虚拟IP地址，由系统动态分配。Service的Cluster IP地址相对于稳定，被创建时就会分配一个IP地址，在销毁该Service之前，这个IP地址都不会再变化的。而Pod生命周期相对较短，新创建的Pod会分配一个新的IP地址。

### 3. 外部访问Service

由于Service对象在Cluster IP Range池中分配到的IP只能在内部访问，所以其他Pod都可以无障碍的访问到它，但如果这个Service作为前端服务，准备为集群外的客户端提供服务，这时候就需要为这个服务提供公共IP。

K8S支持两种对外提供服务的Service的type定义：NodePort和LoadBalancer。

#### 1. NodePort

在定义Service时指定spec.type=NodePort，并指定spec.ports.nodePort的值，系统就会在K8S集群中的每个Node上打开一个主机上的真实端口号。这样，能够访问Node的客户端就都能通过这个端口号访问到内部的Service了。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    name: frontend
spec:
  type: NodePort
  ports: 
  - port: 80
    nodePort: 3001
  selector:
    name: frontend
```

#### 多端口的情况

```yaml
apiVersion: v1
kind: Service
metadata:
	name: tomcat-service
spec:
	ports:
	- port: 8080
		name: service-port
	- port: 8005
		name: shutdown-port
	selector:
		tier: frontend
```

还可以指定NodePort来指定Service暴露服务的端口

```yaml
apiVersion: v1
kind: Service
metadata:
	name: tomcat-service
spec:
	ports:
	- port: 8080
		# 指定node节点暴露的端口
		nodePort: 31002
		name: service-port
	- port: 8005
  	nodePort: 31003
		name: shutdown-port
	selector:
		tier: frontend
```

#### 2. LoadBalancer

如果云服务商支持外接负载均衡器，则可以通过spec.type=LoadBalance定义Service，同时，需要指定负载均衡器的IP地址。使用这种类型需要指定Service的nodePort和clusterIP。

```yaml
apiVersion: v1
kind: Service
metadata: {
	"kind": "Service",
	"apiVersion": "v1",
	metadata: {
		"name": "myService"
	},
	"spec": {
		"type": "LoadBalancer",
		"clusterIP": "xxx.xxx.xxx.xxx",
		"selector": {
			"myApp"
		},
		"ports": [
			{
				"protocol": "TCP",
				"port": 80,
				"targetPort": 9376,
				"nodePort": 3002
			}
		],
	},
	"status": {
		"loadBalancer": {
			"ingress": [
				# 下面这个IP就是云服务商提供的负载均衡器的IP地址
				"ip": "yyy.yyy.yyy.yyy"
			]
		}
	}
}
```

之后，对该Service的访问请求将会通过LoadBalancer转发到后端Pod上去，负载分发的实现方式则依赖于云服务商提供的LoadBalancer的实现机制。

## 1.4.10 Volume（存储卷）

Volume是Pod中能够被多个容器访问的共享目录。K8S的Volume概念与Docker的Volume比较类似，但并不完全相同。K8S中的Volume与Pod生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume中的数据也不会丢失。K8S支持多种类型的Volume，并且一个Pod可以同时使用任意多个Volume。

### 1、Volume类型：

1. EmptyDir：在Pod分配到Node时创建，在同一个Pod中所有容器可以读写EmptyDir重的相同的文件，当Pod从Node上被移除时，EmptyDir中的数据也会永久删除。用途：
    * 临时空间，无需永久保留数据
    * 长时间任务的中间过程CheckPoint临时保存目录
    * 多容器共享目录

2. hostPath：在Pod上挂载宿主机上的文件或目录。用途：
    * 容器应用程序生成的日志文件需要永久保存，可以使用宿主机的高速文件系统进行存储。
    * 需要访问宿主机上Docker引擎哪步数据结构的容器应用，可以通过定义hostPath为宿主机`/var/lib/docker`目录，使容器内部应用可以直接访问Docker的文件系统。

在使用这种类型的Volume时，需要注意：

* 在不同的Node上具有相同配置的Pod可能会因为**宿主机上的目录和文件不同**而导致对Volume上目录和文件的访问结果不一致。
* 如果使用了资源配额管理，则K8S无法将hostPath在宿主机上使用的资源纳入管理。

```yaml
# ......
		spec:
      volumes:
      - name: "persistent-storage"
        hostPath:
          path: "/data"
        volumeMounts:
        - name: "persistent-storage"
          mountPath: "/data"
# ......
```

3. gcePersistentDisk：使用这种类型的Volume表示使用谷歌计算引擎（Google Compute Engine，GCE）上永久磁盘（Persistent Disk，PD）上的文件。与EmptyDir不同，PD上的内容永久保存，当Pod被删除时，PD只是被卸载（Unmount），但不会被删除。需要注意的是，需要先创建一个永久磁盘（PD）才能使用gcePersistentDisk。

使用gcePersistentDisk的限制条件：

* Node节点需要时GCE虚拟机
* 这些虚拟机需要与PD存在于相同的GCE项目和Zone中。

4. awsPersistentDisk：同上。
5. **nfs**：使用NFS（网络文件系统）提供的共享目录挂载到Pod中。在系统中需要一个运行中的NFS系统。

在Pod定义中使用nfs示例：

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: nfs-web
spec:
	containers:
		- name: web
			image: nginx
			ports:
				- name: web
					containerPort: 80
			volumeMounts:
				# name must match the volume name below
				- name: nfs
					mountPath: "/usr/share/nginx/html"
	volumes:
		- name: nfs
			nfs:
				# server是实际的nfs服务器地址
				server: nfs-server
				path: "/"
```

6. 其他挂载方法：iscsi、glusterfs、rbd、gitRepo、secret、persistent VolumeClaim

## 1.4.11 Namespace（命名空间）

Namespace通过将系统内部的对象分配到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户，便于不同的分组在共享使用整个集群的资源的同时，还能被分别管理。K8S集群在启动后，会创建一个名为“default”的Namespace，如果用户不特别指明Namespace，则用户创建的Pod、RC、Service都将被系统创建到“default”的Namespace中。

根据需求创建新的Namespace，可以通过yaml文件来创建：

```yaml
apiVersion: v1
kind: Namespace
metadata: 
	name: development
```

然后在创建Pod时，可以指定Pod所属的Namespace：

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: xxx
	namespace: devlopment
	# ......
```

查询Namespace，如果不指定参数，则默认查询的是“default”名称空间的对象，要查询其他名称空间的对象，需要指定`-n`参数：

```bash
kubectl get pods -n devlopment
```

## 1.4.13 Annotation

Annotation与Label相似，也是使用key/value键值对的形式进行定义。Label具有严格的命名规则，它定义的是K8S对象的元数据，并且勇于Label Selector。Annotation则是用户任意定义的“附加”信息，以便于外部工具进行查找。

Annotation记录的信息包括：

* build、release、Docker镜像等信息，例如：时间戳、镜像hash值、docker registry地址等
* 日志库、监控库、分析库等资源库的地址信息
* 程序调试工具信息，例如：工具名称、版本号等
* 其他信息，例如：团队成员姓名、联系电话等

## 1.4.14 小结

除了以上核心组件，K8S系统中还有其他可配置的资源对象和内部使用的对象，可以查阅K8S的API文档。



# 1.5 K8S总体架构

![](https://borinboy.oss-cn-shanghai.aliyuncs.com/huan/k8s%E6%9E%B6%E6%9E%84.png)

etcd是高可用的key/value存储系统，用于持久化存储集群中所有的资源对象，例如集群中的Node、Service、Pod、RC、Namespace等。API Server则提供了操作etcd的封装接口API，以REST方式提供服务，这些API基本上都是集群中资源对象的增删改查及监听资源变化的接口。API Server是连接其他所有服务组件的枢纽。

## 1.5.1 K8S相关生命周期

### 1. 创建RC生命周期

kubectl创建一个RC，通过API Server写入到etcd，Controller Manager监听API Server，监听到创建RC的事件，分析当前集群，如果没有该RC对应的Pod实例，则生成对应的Pod对象，并通过API Server写入etcd中，该事件被Scheduler发现，通过一系列调度流程，分配到一个Node节点上，这个过程可称为**绑定（Pod Binding）**，然后又通过API Server将这结果写入到etcd中，随后，目标Node上运行的Kubelet进程通过API Server监测到这个“新生的”Pod并且按照它的定义，启动Pod，直至销毁。

### 2. Pod请求生命周期

kubectl提交一个映射到Pod的Service的创建请求，Controller Manager会通过Label查询到相关联的Pod实例，然后**生成Service的Endpoints信息**并通过API Server写入到etcd中。所有Node上运行的**Proxy进程通过API Server查询并监听Service对象与其对应的Endpoints信息**，建立一个软件方式的负载均衡器来实现Service访问到后端Pod的流量转发功能。

## 1.5. 2 K8S组件功能总结

API Server：提供资源对象的唯一操作入口。“全量查询，监听变化”资源数据。API Server内部有一套完备的安全机制，API Server在收到一个REST请求后，会首先执行认证、授权和准入控制的相关逻辑，过滤非法请求，再进行发送。

Controller Manager：集群内部管理控制中心，实现K8S集群的故障检测和恢复的自动化工作。

Scheduler：集群中的调度器，负责Pod在集群（Node）节点中的调度分配。

* 预选策略
* 优选策略

Kubelet：负责本Node节点上的Pod增删改查、监控等全生命周期管理，同时定时上报本Node的状态信息到API Server中。

Kube Proxy：实现了Service的代理及软件模式的负载均衡器。**通过API Server查询并监听Service对象与其对应的Endpoints信息，建立软件方式的负载均衡器。**

kubectl：K8S客户端命令行管理工具。

