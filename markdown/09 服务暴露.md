## 服务暴露
K8S的DNS实现了服务在集群"内"被自动发现，那如何使得服务在K8S集群"外"被使用和访问？
* 使用NodePort型Service
  * 注意：无法使用kube-proxy的IPvs模型，只能使用IPtables模型
* 使用Ingress资源
  * 注意：Ingress只能调度并暴露7层应用，特指http和https协议

我们在7-21上测试一下，集群外部访问集群内部的nginx服务
```shell
[root@hdss7-21 ~]# curl nginx-dp.kube-public.svc.cluster.local
curl: (6) Could not resolve host: nginx-dp.kube-public.svc.cluster.local; Unknown error
```
可以看到，外部是无法访问的，因此，我们就需要一种方式，使得外部能够访问内部的资源，否则我们部署K8S也就没有什么特别大的意义。

### Ingress
Ingress是K8S API的标准资源类型之一，也是一种核心资源，它其实就是一组基于域名和URL路径，把用户的请求转发至指定Service资源的规则。

Ingress可以将集群外部的请求流量，转发至集群内部，从而实现"服务暴露"。

Ingress控制器是能够调度为Ingress资源监听某套接字，然后会根据Ingress规则匹配机制路由调度流量的一个组件。

> 说白了，Ingress就是nginx+一段go脚本

常用的Ingress控制器的实现软件
* Ingress-nginx
* HAProxy
* **Traefik**



## 测试服务暴露

### 使用IPtables（不做演示）
修改**两台机器**的配置文件，并重启两台机器的相关服务
```shell
[root@hdss7-21 ~]# vim /opt/kubernetes/server/bin/kube-proxy.sh
#!/bin/sh
./kube-proxy \
  --cluster-cidr 172.7.0.0/16 \
  --hostname-override hdss7-21.host.com \
  --proxy-mode=iptabels \
  --ipvs-scheduler=rr \
  --kubeconfig ./conf/kube-proxy.kubeconfig
```
> 使用iptables，IPvs只能用rr

重启kube-proxy
```shell
[root@hdss7-21 ~]# supervisorctl restart kube-proxy-7-21
[root@hdss7-22 ~]# supervisorctl restart kube-proxy-7-22
```
查看和删除原来的ipvsadm规则
```shell
[root@hdss7-21 ~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 10.4.7.21:6443               Masq    1      0          0
  -> 10.4.7.22:6443               Masq    1      0          0
TCP  192.168.0.2:53 nq
  -> 172.7.21.3:53                Masq    1      0          0
TCP  192.168.0.2:9153 nq
  -> 172.7.21.3:9153              Masq    1      0          0
UDP  192.168.0.2:53 nq
  -> 172.7.21.3:53                Masq    1      0          0

[root@hdss7-21 ~]# ipvsadm -D -t 192.168.0.1:443
[root@hdss7-21 ~]# ipvsadm -D -t 192.168.0.2:53
[root@hdss7-21 ~]# ipvsadm -D -t 192.168.0.2:9153
[root@hdss7-21 ~]# ipvsadm -D -u 192.168.0.2:53
```

创建新的svc资源
```yaml
[root@hdss7-21 ~]# vim nginx-ds-svc.yaml
apiVersion: v1
kind: Service
metadata:
  lables:
    app: nginx-ds
  name: nginx-ds
  namespace: default

spec:
  ports:
  - port: 80
    protocol: TCP
    nodePort: 8000
  selector:
    app: nginx-ds
  sessionAffinity: None
  type: NodePort
```
使用`kubectl apply -f`使配置生效
```shell
[root@hdss7-21 ~]# kubectl apply -f nginx-ds-svc.yaml
```
然后就可以在宿主机上访问`10.4.7.21:8000`和`10.4.7.21:8000`访问到K8S集群内部的服务。
根据上述实现可以发现，如果我们用nodePort型的Service，那么其实就是通过kube proxy去写iptables规则。

traefik资源配置清单参考：https://github.com/traefik/traefik/tree/v1.7/examples/k8s
* deployment：一台宿主机跑多份
* DaemonSet：一台宿主机跑一个副本

### nginx七层反向代理

配置nginx七层反向代理配置文件

```text
upstream default_backend_traefik {
    server 10.4.7.21:81    max_fails=3 fail_timeout=10s;
    server 10.4.7.22:81    max_fails=3 fail_timeout=10s;
}
server {
    server_name *.od.com;

    location / {
        proxy_pass http://default_backend_traefik;
        proxy_set_header Host       $http_host;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
    }
}
```
配置nginx反向代理以后，nginx就不用再动了，如果需要调度流量7层规则怎么办？

只需要声明ingress资源配置清单，凡是走向`*.od.com`的流量，全部转发到IngressController-->Ingress类型资源,而Ingress中又定义了spec/rules/规则，去找traefik-ingress-service。它又是怎么找到pod的呢？通过node-selector找到k8s-app: traefik-ingress，然后找到对应的k8s-app的标签。

svc.yaml

```yaml
  selector:
    k8s-app: traefik-ingress
```

ds.yaml

```yaml
      labels:
        k8s-app: traefik-ingress
```

等所有都配置和部署完成之后，就可以查看页面。

![](https://borinboy.oss-cn-shanghai.aliyuncs.com/huan/20211022125936.png)

## 总结
* Ingress控制器 --> 一个简化版对nginx（调度流量）+ go脚本（动态识别yaml文件）（如果变化自动重新加载）
* Traefik --> 实现了ingress控制器的一个软件

traefik.od.com --> traefik-ingress-service --> Service --> selector --> k8s-app: traefik-ingress
daemonSet --> labels --> k8s-app: traefik-ingress

